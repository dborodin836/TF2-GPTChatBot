[GENERAL]
TF2_LOGFILE_PATH=H:\Programs\Steam\steamapps\common\Team Fortress 2\tf\console.log
OPENAI_API_KEY=

[COMMANDS]
GPT_COMMAND=!gpt3
CHATGPT_COMMAND=!cgpt
CLEAR_CHAT_COMMAND=!clear
RTD_COMMAND=!rtd
GPT4_ADMIN_ONLY=False

[RCON]
RCON_HOST=127.0.0.1
RCON_PASSWORD=password
RCON_PORT=42465

[CHAT]
; Type: bool [True/False]
;
; Before (ENABLE_SHORTENED_USERNAMES_RESPONSE=True):
; [username] <response goes here>
; After (ENABLE_SHORTENED_USERNAMES_RESPONSE=False):
; <response goes here>
;
; Default = True
ENABLE_SHORTENED_USERNAMES_RESPONSE=True

; MUST include $username
; Type: string
;
; Before (SHORTENED_USERNAMES_FORMAT="[$username] "):
; [username] <response goes here>
; After (SHORTENED_USERNAMES_FORMAT="Response for $username: "):
; Response for username: <response goes here>
;
; Default = "[$username] "
SHORTENED_USERNAMES_FORMAT="[$username] "

; Type: integer
;
; Before (SHORTENED_USERNAME_LENGTH=12):
; [longusername] <response goes here>
; After (SHORTENED_USERNAME_LENGTH=6):
; [longus..] <response goes here>
;
; Default = 12
SHORTENED_USERNAME_LENGTH=12

; Time in seconds
; Type: float
;
; Default: 1.3
DELAY_BETWEEN_MESSAGES=1.3

[MISC]
SOFT_COMPLETION_LIMIT=128
HARD_COMPLETION_LIMIT=300
TOS_VIOLATION = 0
; Adds a custom prompt after the SOFT_COMPLETION_LIMIT message. This affects !gpt3, !gpt4, !gpt4l, !cgpt commands
; So it will look something like:
; <prompt goes here> Answer in less than 128 chars! <custom prompt goes here>
; Type: string
;
; Example 1 (CUSTOM_PROMPT=Make your response a joke.):
; > !gpt3 How ai works?
; < AI works by pretending to be intelligent, just like my toaster pretending to be a time machine.
; Example 2 (CUSTOM_PROMPT=):
; > !gpt3 How ai works?
; < AI works by combining data, algorithms, and computing power to create systems that can learn,
;   reason, and make decisions like humans.
; Example 3 (CUSTOM_PROMPT=Add something like 'How you didn't know that ( ͡° ͜ʖ ͡°)' this to your response):
; > !gpt3 What's 23 * 3?
; < 23 * 3 = 69. How you didn't know that ( ͡° ͜ʖ ͡°)
;
; Default =
CUSTOM_PROMPT =

[STATS]
ENABLE_STATS=0
STEAM_WEBAPI_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

[CUSTOM-MODEL-GENERAL]
ENABLE_CUSTOM_MODEL=0
CUSTOM_MODEL_COMMAND=!ai
CUSTOM_MODEL_CHAT_COMMAND=!chat

; 127.0.0.1:5000 or your-uri-here.trycloudflare.com
CUSTOM_MODEL_HOST=127.0.0.1:5000

; For more settings, follow this links.
; https://platform.openai.com/docs/api-reference/completions/create
; OR
; https://github.com/oobabooga/text-generation-webui/wiki/12-%E2%80%90-OpenAI-API#examples
;
; Example:
; CUSTOM_MODEL_SETTINGS = {"echo": true, "max_tokens": 200, "temperature": 2.0, "stop": ["FOO", "BAR"], "logit_bias": {"50256": -100} }
CUSTOM_MODEL_SETTINGS =

[EXPERIMENTAL]
; Unlike the standard 'Fire-and-Forget' queue which sends messages without acknowledging
; their delivery, the confirmable queue adds reliability by ensuring that each message is
; successfully delivered. It also integrates additional checks to ascertain if a user
; has been muted by the game, preventing unnecessary message transmission while muted.
; It is considered experimental and may be more error-prone than the standard approach.
; Use with caution, as it may introduce unexpected behavior or performance issues.
; Type: bool [True/False]
;
; Default = False
CONFIRMABLE_QUEUE=False

[FUN]
; 0 - disabled, 1 - rickroll, 2 - random youtube meme
RTD_MODE = 0
